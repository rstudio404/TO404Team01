---
title: "Asymmetric Traffic"
author: "Josh Hartman"
date: "November 25, 2020"
output: 
  html_document:
    number_sections: true
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Explore the data
```{r, cache=TRUE}
# attach package readr to read zip files
#library("readr")
#Jan <- read_csv("201901-citibike-tripdata.csv.zip")
#Feb <- read_csv("201902-citibike-tripdata.csv.zip")
#Mar <- read_csv("201903-citibike-tripdata.csv.zip")
#Apr <- read_csv("201904-citibike-tripdata.csv.zip")
#May <- read_csv("201903-citibike-tripdata.csv.zip")
#Jun <- read_csv("201903-citibike-tripdata.csv.zip")
#Jul <- read_csv("201903-citibike-tripdata.csv.zip")
#Aug <- read_csv("201903-citibike-tripdata.csv.zip")
#Sep <- read_csv("201903-citibike-tripdata.csv.zip")
#Oct <- read_csv("201903-citibike-tripdata.csv.zip")
#Nov <- read_csv("201903-citibike-tripdata.csv.zip")
#Dec <- read_csv("201903-citibike-tripdata.csv.zip")
#combine all files
#combo <- rbind(Jan,Feb,Mar,Apr,May,Jun,Jul,Aug,Sep,Oct,Nov,Dec)
#take a sample of to work with by attaching dplyr package and taking a 5% sample
#library("dplyr")
#citibikesample <- sample_frac(combo, 0.05)
#convert to csv
#write.csv(citibikesample,"citibikesample.csv")
#viewing structure of sample

citibikesample <- read.csv("sample19.csv", stringsAsFactors = TRUE)


```
# Clean Data

```{r}
# convert relevant fields to factors

citibikesample$bikeid <- as.factor(citibikesample$bikeid)

#convert 1 and 2 to m and f

citibikesample$gender <- ifelse(citibikesample$gender == 1| citibikesample$gender == "M", "M",ifelse(citibikesample$gender == 2| citibikesample$gender ==  "F","F", NA))
  
citibikesample$gender<- as.factor(citibikesample$gender)



#revisualize
str(citibikesample)
```

# Load Packages
```{r }
#attach most packages here
library("ggplot2")
# attach dplyr
library("dplyr")
# attach scales
library("scales")
library("leaflet")
library("sp")
library("ggmap")
#D. Kahle and H. Wickham. ggmap: Spatial Visualization with ggplot2. The R Journal, 5(1), 144-161. URL http://journal.r-project.org/archive/2013-1/kahle-wickham.pdf

library("maptools")
library("httr")
library("gganimate")
```

# Asymmetry Maps
```{r}
#setup for following r chunks

#making a new data.frame that maps count of visits to start staion id
freqstartstationid <- count(citibikesample, start.station.id)
freqstartstationid<- freqstartstationid[order(freqstartstationid$n, decreasing = TRUE),]

#merging this df with existing one by start station id, will add new column n which is the count of the start station id in the original df
freqstartcitibikesample <- merge(citibikesample, freqstartstationid, by.x = "start.station.id", by.y = "start.station.id")
freqstartcitibikesample$n -> freqstartcitibikesample$start.station.count

#taking top occuring start stations and making them a new df, topstartstation
#because the top start stations occur thousands of times, used count of start stations * 2 to get number of rows to include
topstartstation <- head(freqstartcitibikesample[order(freqstartcitibikesample$n, decreasing = TRUE),], n= sum(freqstartstationid[1:3,2]))
topstartstation <- topstartstation[,c(1,6:7, 16:17)]
#get tail of df to display lower occuring start stations and name bottomstartstation
bottomstartstation <- tail(freqstartcitibikesample[order(freqstartcitibikesample$n, decreasing = TRUE), ], n=10)
#repeat for end stations
freqendstationid <- count(citibikesample, end.station.id)
#repeating merge but by end station id
freqendcitibikesample <- merge(citibikesample, freqendstationid, by.x = "end.station.id", by.y = "end.station.id")
#taking top occuring end stations and making them a new df, topendstation
#because the top end stations occur thousands of times, used count of end stations * 2 to get number of rows to include
topendstation <- head(freqendcitibikesample[order(freqendcitibikesample$n, decreasing = TRUE),], n= max(freqendcitibikesample$n)*2)
#get tail of this df to display lower occuring end stations
bottomendstation <- tail(freqendcitibikesample[order(freqendcitibikesample$n, decreasing = TRUE), ], n=10)

#make df to show assym
#make data frame that has lat and long for each start station
locationinfo <- freqstartcitibikesample[,c(1,6,7)]

#merging counts for start and end stations and then merging with location info
assymstation <- merge(freqstartstationid, freqendstationid, by.x = "start.station.id", by.y = "end.station.id")
assymstation_location <- merge(locationinfo, assymstation, by.x = "start.station.id", by.y = "start.station.id")
#removing duplicate rows
assymtraffic <- assymstation_location[!duplicated(assymstation_location$start.station.id),]
#making it easier to understand
assymtraffic$start.station.id -> assymtraffic$station.id
assymtraffic$n.x -> assymtraffic$count.start.station
assymtraffic$n.y -> assymtraffic$count.end.station
#getting rid of these columns
assymtraffic$start.station.id <- NULL
assymtraffic$n.x <- NULL
assymtraffic$n.y <- NULL
#making these vectors integers
assymtraffic$count.start.station <- as.integer(assymtraffic$count.start.station)
assymtraffic$count.end.station <- as.integer(assymtraffic$count.end.station)
#creating new column that determines surplus or deficit by seeing the difference between occurences of start and end stations
assymtraffic$difference <- c(assymtraffic$count.end.station - assymtraffic$count.start.station)
#order so that difference is decreasing
sortedassymstation <- assymtraffic[order(assymtraffic$difference, decreasing = TRUE),]
summary(sortedassymstation$difference)
```

## Top 10 Stations with Surplus
```{r}

#making df with stations that have surplus
surplusstations <- sortedassymstation[sortedassymstation$difference > 0, ]

#get top 10 stations
topsurplusstations <- head(surplusstations, n = 10)
topsurplusstations$start.station.latitude <- as.numeric(topsurplusstations$start.station.latitude)
topsurplusstations$start.station.longitude <- as.numeric(topsurplusstations$start.station.longitude)
#using sortedassymstation df to make a leaflet showing stations used as an end station more than as a start station
leaflet(topsurplusstations) %>%
  addTiles()%>% 
  addMarkers(data = topsurplusstations, lng = ~start.station.longitude, lat = ~start.station.latitude, popup = paste("Station ID:", topsurplusstations$station.id, "<br>", "Surplus:", topsurplusstations$difference)) %>% #adding markers with station id displayed and the surplus
  setView(-73.96,40.75, zoom = 9)
```

## Top 10 Stations with Deficit
```{r}
#making df with stations that have surplus
deficitstations <- sortedassymstation[sortedassymstation$difference < 0, ]

#get top 10 stations
bottomdeficitstations <- tail(deficitstations, n = 10)
bottomdeficitstations$start.station.longitude <- as.numeric(bottomdeficitstations$start.station.longitude)
#using sortedassymstation df to make a leaflet showing stations used as an end station more than as a start station
leaflet(bottomdeficitstations) %>%
  addTiles()%>% #adding map tiles
  addMarkers(data = bottomdeficitstations, ~start.station.longitude, ~start.station.latitude, popup = paste("Station ID:", bottomdeficitstations$station.id, "<br>", "Deficit:", bottomdeficitstations$difference)) %>% #adding markers with station id displayed and the surplus
  setView(-73.96,40.75, zoom = 9)
```









When observing these maps, we see that the stations with the largest defjcits are clustered around central park while the stations with the largest surpluses are around the southern end of Manhattan. The station with the highest deficit, station `r bottomdeficitstations$station.id[10]` is actually not far from the station with the highest surplus, `r topsurplusstations$station.id[1]`. Overall, this data suggests that it may be worthwhile to allocate larger bike reserves to these high deficit stations by pulling some bikes from the high surplus stations.

# Asymmetry by Time
## Top 5 Surplus and Top 5 Deficits By Day
```{r}

# lubridate package to get date times
library("lubridate"); library("chron");library("timeDate")

#getting relevant vectors from sample data
datafortime <- citibikesample[,c(2:4,6:8,10:13)]

#ymdhms format
datafortime$starttime <- ymd_hms(datafortime$starttime)
datafortime$stoptime <- ymd_hms(datafortime$stoptime)

#make list of holidays 
holidaylist <- c("USChristmasDay","USGoodFriday","USIndependenceDay","USLaborDay",
    "USNewYearsDay","USThanksgivingDay") 

#get dates of holidays for 2019
myholidays  <- floor_date(ymd(as.character(holiday(2019,holidaylist)), "day"))

#omit nas
na.omit(myholidays) -> myholidays

#make a df to later search for name of holiday associated with date
Holidays <- data.frame(unlist(holidaylist, recursive = TRUE))
Holidaydf <- cbind(Holidays,myholidays)

#create new vectors for different time increments
datafortime$month <- month(datafortime$starttime)
datafortime$day <- floor_date(datafortime$starttime, "day")
datafortime$hour <- hour(datafortime$starttime)
datafortime$weekday <- weekdays(datafortime$starttime)
datafortime$roundedhour <- floor_date(datafortime$starttime, "hour")

#make a vector that determines whether a date is a holiday and returns the name of the holiday if it is, else "Not Holiday"
datafortime$holiday <- ifelse(datafortime$day == myholidays[1]|datafortime$day == myholidays[2]|datafortime$day == myholidays[3]|datafortime$day == myholidays[4]|datafortime$day == myholidays[5]|datafortime$day == myholidays[6], Holidaydf[match(datafortime$day, Holidaydf$myholidays),1],"Not Holiday")

#make a factor
datafortime$holiday <- as.factor(datafortime$holiday)

#get counts for start and end station ids by day
startstationbyday <- count(datafortime, start.station.id, day)
endstationbyday <- count(datafortime, end.station.id, day)

#combine df's by station id and day
combinedcountbyday <- merge(startstationbyday, endstationbyday, by.x = c("start.station.id","day"), by.y = c("end.station.id","day"))

#make vector of surplus(deficit)
combinedcountbyday$dif <- c(combinedcountbyday$n.y - combinedcountbyday$n.x)

#define day as a date
combinedcountbyday$day <- as.Date(combinedcountbyday$day)
combinedcountbyday <- combinedcountbyday[order(combinedcountbyday$dif, decreasing = TRUE),]
#stations with the 5 highest surplus
topcombinedcountbyday <- combinedcountbyday[combinedcountbyday$start.station.id == combinedcountbyday[1,1]| combinedcountbyday$start.station.id == combinedcountbyday[2,1]|combinedcountbyday$start.station.id == combinedcountbyday[3,1]|combinedcountbyday$start.station.id == combinedcountbyday[4,1]|combinedcountbyday$start.station.id == combinedcountbyday[5,1],]

#stations with the 5 highest deficits
bottomcombinedcountbyday <- combinedcountbyday[combinedcountbyday$start.station.id == combinedcountbyday[nrow(combinedcountbyday),1]| combinedcountbyday$start.station.id == combinedcountbyday[nrow(combinedcountbyday)-1,1]|combinedcountbyday$start.station.id == combinedcountbyday[nrow(combinedcountbyday)-2,1]|combinedcountbyday$start.station.id == combinedcountbyday[nrow(combinedcountbyday)-3,1]|combinedcountbyday$start.station.id == combinedcountbyday[nrow(combinedcountbyday)-4,1],]
#bind the rows of top and bottom 5 stations
topbottombyday <- rbind(topcombinedcountbyday,bottomcombinedcountbyday)


#make a line plot that plots surplus/deficit by day
surplusplot <- ggplot(topbottombyday, aes(x= day, y= dif))+ geom_line(color = "darkgreen")+facet_wrap("start.station.id")+labs(x = "Date", y = "Difference")+theme(axis.text.x = element_text(angle=90, hjust=1))

#group_by(combinedcountbyday, day)%>%
 # summarise(
  #  Dif = median(dif, na.rm = TRUE)
  #)%>%
  #ggplot(aes(x= day)) + geom_col(aes(y=Dif), fill = "darkgreen") +theme(axis.text.x = element_text(angle=90, hjust=1))


#make barplot that shows surplus or deficit by day for top 5 and bottom 5 stations
surplusbarplot <- ggplot(topbottombyday, aes(x= day)) + geom_col(aes(y=dif), fill = "darkgreen") + facet_wrap("start.station.id") +theme(axis.text.x = element_text(angle=90, hjust=1))
surplusbarplot


```










From this we can see that the many of the stations with the highest surplus on a given day are also the stations with the highest deficit on a given day.This suggests that these stations are in high areas of traffic that varies in direction of flow day by day. In terms of implications, when allocating higher bike reserves to high deficit locations, these stations are going to change day to day - a station that today has the largest deficit may tommorow have the largest surplus. 

## Monthly
```{r}
#make new df that counts start and end station by month
startstationbymonth <- count(datafortime, start.station.id, month)
endstationbymonth <- count(datafortime, end.station.id, month)

#combine df's by station id and month
combinedcountbymonth <- merge(startstationbymonth, endstationbymonth, by.x = c("start.station.id","month"), by.y = c("end.station.id","month"))

#find surplus(deficit) and make a new vector
combinedcountbymonth$dif <- c(combinedcountbymonth$n.y - combinedcountbymonth$n.x )

#make a plot that shows median surplus(deficit) by month for all stations
group_by(combinedcountbymonth, month)%>%
summarise(
  Dif = median(dif, na.rm = TRUE))-> medianmonth

medianmonth <- as.data.frame(medianmonth)

ggplot(data = medianmonth, aes(y= Dif))+geom_col(aes(x = month), fill = "darkgreen")+xlab("Month") + ylab("Surplus (Deficit)") +ggtitle("Median Surplus(Deficit) by Month")+scale_x_discrete(limit = c(1,2,3,4,5,6,7,8,9,10,11,12)) 

#get count of each station for use as "index" to get a specified number of stations
monthlycountedstation <- count(combinedcountbymonth, combinedcountbymonth$start.station.id)

combinedcountbymonth[order(combinedcountbymonth$dif, decreasing = TRUE),1]-> combinedcountbymonth1 
#stations with the 5 highest surplus
topcombinedcountbymonth <- combinedcountbymonth[combinedcountbymonth$start.station.id == combinedcountbymonth1[1]| combinedcountbymonth$start.station.id == combinedcountbymonth1[2]|combinedcountbymonth$start.station.id == combinedcountbymonth1[3]|combinedcountbymonth$start.station.id == combinedcountbymonth1[4]|combinedcountbymonth$start.station.id == combinedcountbymonth1[5],]

#stations with the 5 highest deficits
bottomcombinedcountbymonth <- combinedcountbymonth[combinedcountbymonth$start.station.id == combinedcountbymonth1[length(combinedcountbymonth1)]| combinedcountbymonth$start.station.id == combinedcountbymonth1[length(combinedcountbymonth1)-1]|combinedcountbymonth$start.station.id == combinedcountbymonth1[length(combinedcountbymonth1)-2]|combinedcountbymonth$start.station.id == combinedcountbymonth1[length(combinedcountbymonth1)-3]|combinedcountbymonth$start.station.id == combinedcountbymonth1[length(combinedcountbymonth1)-4],]
#bind the rows of top and bottom 5 stations
topbottombymonth <- rbind(topcombinedcountbymonth,bottomcombinedcountbymonth)


#make a plot that shows surplus(deficit) by month for 5 stations with highest surplus and 5 stations with highest deficit
#head(combinedcountbymonth, n= sum(monthlycountedstation[1:10,2]))%>%
ggplot(data = topbottombymonth, aes(x= month, y= dif))+geom_col(fill = "darkgreen")+facet_wrap("start.station.id")+scale_x_discrete(limit = c(1,2,3,4,5,6,7,8,9,10,11,12)) 

#aggregate max dif by month
as.data.frame(aggregate(combinedcountbymonth$dif, by = list(Month = combinedcountbymonth$month), FUN = max))-> monthsurplus

#make a for loop to go through and get the stations tied to the max surplus
for(x in 1:nrow(monthsurplus)){
monthsurplus$stations[x] <- as.character(combinedcountbymonth[combinedcountbymonth$dif == monthsurplus$x[x] & combinedcountbymonth$month == monthsurplus$Month[x], 1])
}
#reconvert to factor
monthsurplus$stations <- as.factor(monthsurplus$stations)

#merge with lat/lon data and rename vector to "Surplus" 
unique(merge(monthsurplus, citibikesample[,c(4,6:7)], by.x = "stations", by.y = "start.station.id"))%>%rename( Surplus = x) -> monthsurplus


#aggregate min dif by month
as.data.frame(aggregate(combinedcountbymonth$dif, by = list(Month = combinedcountbymonth$month), FUN = min))-> deficitmonth

#make a for loop to go through and get the stations tied to the largest deficits
for(x in 1:nrow(deficitmonth)){
deficitmonth$stations[x] <- as.character(combinedcountbymonth[combinedcountbymonth$dif == deficitmonth$x[x] & combinedcountbymonth$month == deficitmonth$Month[x], 1])
}

#reconvert to factor
deficitmonth$stations <- as.factor(deficitmonth$stations)

#merge with lat/lon data and rename vector to "Deficit" 
unique(merge(deficitmonth, citibikesample[,c(4,6:7)], by.x = "stations", by.y = "start.station.id"))%>%rename( Deficit = x) -> deficitmonth

```




Our sample data demonstrate that months `r medianmonth[medianmonth$Dif != 0,1]` have a nonzero median deficits. While average data may show differently, it is best to view median data to avoid outliers that may misrepresent the data.  `r medianmonth[medianmonth$Dif == min(medianmonth$Dif),1]` appears to have the greatest median deficit indicating that extra emphasis should go toward addressing asymmetric traffic during this month. However, individual stations have their own trends. Looking at the top 5 surplus and deficit stations by month, we see that most stations tend to maintain a deficit or surplus standing for each month, with a only few having both large surpluses and large dficits. 


These graphs are representative of the stations with the largest 5 surpluses and largest 5 deficits by month. In contrast to the daily data, most of these stations consistently have a cumulative monthly surplus or deficit as evidenced by the directionaly of the bars and the minimal amount of overlap between top 5 surpluses and top 5 deficits. These high consistent surplus stations may be able to take a smaller bike reserve by month on average while the consistent deficit stations may be able to take a higher bike reserve by month on average.



### Top Surplus Stations By Month
```{r}
#coordinates for some are the same, introducing some random noise to separate out a bit while keeping non duplicated lat/lon untouched
  monthsurplus$start.station.latitude1 <- ifelse(duplicated(monthsurplus$start.station.latitude) == TRUE & duplicated(monthsurplus$start.station.longitude)==TRUE, jitter(monthsurplus$start.station.latitude, factor = 0.01, amount = 0.001), monthsurplus$start.station.latitude)

library(viridis)
#make color palette for months
pal <- colorFactor(palette = rainbow(12), domain = monthsurplus$Month)
#map max surplus by month
  leaflet(monthsurplus)%>%
  addTiles() %>%
  addCircleMarkers(lng = ~start.station.longitude, lat = ~start.station.latitude1, color = ~pal(Month),  radius = 5, opacity = 20, popup = paste("Station ID:", monthsurplus$stations, "<br>","Surplus:", monthsurplus$Surplus, "<br>", "Month:", monthsurplus[,2]))%>%
    addLegend(position = "bottomright", pal = pal, values = monthsurplus$Month)%>%
  setView(-73.96,40.75, zoom = 9) 
  
 monthsurplus[order(monthsurplus$Month, decreasing = FALSE),]
```



### Top Deficit Stations By Month
```{r}
#coordinates for four are the same, introducing some random noise to separate out a bit will
  deficitmonth$start.station.latitude1 <- ifelse(duplicated(deficitmonth$start.station.latitude) == TRUE & duplicated(deficitmonth$start.station.longitude)==TRUE, jitter(deficitmonth$start.station.latitude, factor = 0.01, amount = 0.001), deficitmonth$start.station.latitude)

#make color palette for months
pal <- colorFactor(palette = rainbow(12), domain = deficitmonth$Month)
#map max deficit by month
  leaflet(deficitmonth)%>%
  addTiles() %>%
  addCircleMarkers(lng = ~start.station.longitude, lat = ~start.station.latitude1, color = ~pal(Month),  radius = 5, opacity = 20, popup = paste("Station ID:", deficitmonth$stations, "<br>","Deficit:", deficitmonth$Deficit, "<br>", "Month:", deficitmonth[,2]))%>%
    addLegend(position = "bottomright", pal = pal, values = deficitmonth$Month)%>%
  setView(-73.96,40.75, zoom = 9) 
  
  deficitmonth[order(deficitmonth$Month, decreasing = FALSE),]
```


This map shows which stations are the best candidates to provide a higher bike reserves to on average by month. Please see the map for a table version of this map in case the overlap makes it difficult to discern the months.



## Weekday
```{r}
#make new df that counts start and end station by weekday
startstationbyweekday <- count(datafortime, start.station.id, weekday)
endstationbyweekday <- count(datafortime, end.station.id, weekday)

#combine df's by station id and weekday
combinedcountbyweekday <- merge(startstationbyweekday, endstationbyweekday, by.x = c("start.station.id","weekday"), by.y = c("end.station.id","weekday"))

#find surplus(deficit) and make a new vector
combinedcountbyweekday$dif <- c(combinedcountbyweekday$n.y - combinedcountbyweekday$n.x)

#make weekday an ordered factor so display it Monday to Sunday
combinedcountbyweekday$weekday <- ordered(combinedcountbyweekday$weekday, levels = c("Monday", "Tuesday", "Wednesday", "Thursday","Friday", "Saturday", "Sunday"))

#make class vector
combinedcountbyweekday$class <- ifelse(combinedcountbyweekday$weekday == "Saturday" |combinedcountbyweekday$weekday == "Sunday", "Weekend", "Weekday")

#make a plot of mean surplus(deficit) by weekday for all stations
group_by(combinedcountbyweekday, class, weekday)%>%
summarise(
  Dif = mean(dif, na.rm = TRUE)) -> weekdayaverage

#make it a df
weekdayaverage <- as.data.frame(weekdayaverage)

#group by class and weekday then summarize median surplus(deficit)
group_by(combinedcountbyweekday, class, weekday)%>%
summarise(
  Dif = median(dif, na.rm = TRUE)) -> weekdaymedian

#make it a df
weekdaymedian <- as.data.frame(weekdaymedian)

#plot median data
ggplot(data = weekdaymedian, aes(x = weekday, y= Dif))+geom_col(aes(fill = class))+xlab("Weekday") + ylab("Surplus (Deficit)")+ggtitle("Median Surplus(Deficit) by Weekday") 


#make a vector of stations in decreasing surplus(deficit) order
combinedcountbyweekday[order(combinedcountbyweekday$dif, decreasing = TRUE),1]-> combinedcountbyweekday1 

#stations with the 5 highest surplus
topcombinedcountbyweekday <- combinedcountbyweekday[combinedcountbyweekday$start.station.id == combinedcountbyweekday1[1]| combinedcountbyweekday$start.station.id == combinedcountbyweekday1[2]|combinedcountbyweekday$start.station.id == combinedcountbyweekday1[3]|combinedcountbyweekday$start.station.id == combinedcountbyweekday1[4]|combinedcountbyweekday$start.station.id == combinedcountbyweekday1[5],]

#stations with the 5 highest deficits
bottomcombinedcountbyweekday <- combinedcountbyweekday[combinedcountbyweekday$start.station.id == combinedcountbyweekday1[length(combinedcountbyweekday1)]| combinedcountbyweekday$start.station.id == combinedcountbyweekday1[length(combinedcountbyweekday1)-1]|combinedcountbyweekday$start.station.id == combinedcountbyweekday1[length(combinedcountbyweekday1)-2]|combinedcountbyweekday$start.station.id == combinedcountbyweekday1[length(combinedcountbyweekday1)-3]|combinedcountbyweekday$start.station.id == combinedcountbyweekday1[length(combinedcountbyweekday1)-4],]

#bind the rows of top and bottom 5 stations
topbottombyweekday <- rbind(topcombinedcountbyweekday,bottomcombinedcountbyweekday)



#show top 5 surplus and deficit stations by weekday and split by station
ggplot(data= topbottombyweekday, aes(x= weekday, y= dif))+geom_col(fill = "darkgreen")+facet_wrap("start.station.id")+theme(axis.text.x = element_text(angle=90, hjust=1))


#aggregate min by weekday
as.data.frame(aggregate(combinedcountbyweekday$dif, by = list(Weekday = combinedcountbyweekday$weekday), FUN = max))-> surpluscombinedcountbyweekdaycoord

#for loop to map stations to data
for(x in 1:nrow(surpluscombinedcountbyweekdaycoord)){
surpluscombinedcountbyweekdaycoord$stations[x] <- as.character(combinedcountbyweekday[combinedcountbyweekday$dif == surpluscombinedcountbyweekdaycoord$x[x] & combinedcountbyweekday$weekday == surpluscombinedcountbyweekdaycoord$Weekday[x], 1])
}

#make factor again
surpluscombinedcountbyweekdaycoord$stations <- as.factor(surpluscombinedcountbyweekdaycoord$stations)

#merge unique rows with lat/lon data and rename vector
unique(merge(surpluscombinedcountbyweekdaycoord, citibikesample[,c(4,6:7)], by.x = "stations", by.y = "start.station.id"))%>% rename( Surplus = x) -> surpluscombinedcountbyweekdaycoord

#aggregate min by weekday
as.data.frame(aggregate(combinedcountbyweekday$dif, by = list(Weekday = combinedcountbyweekday$weekday), FUN = min))-> deficitcombinedcountbyweekdaycoord

#for loop to map stations to data
for(x in 1:nrow(deficitcombinedcountbyweekdaycoord)){
deficitcombinedcountbyweekdaycoord$stations[x] <- as.character(combinedcountbyweekday[combinedcountbyweekday$dif == deficitcombinedcountbyweekdaycoord$x[x] & combinedcountbyweekday$weekday == deficitcombinedcountbyweekdaycoord$Weekday[x], 1])
}

#make factor again
deficitcombinedcountbyweekdaycoord$stations <- as.factor(deficitcombinedcountbyweekdaycoord$stations)

#merge unique rows with lat/lon data and rename vector
unique(merge(deficitcombinedcountbyweekdaycoord, citibikesample[,c(4,6:7)], by.x = "stations", by.y = "start.station.id"))%>% rename( Deficit = x) -> deficitcombinedcountbyweekdaycoord


```








While average surplus(deficit) data demonstrates that there is a stronger average deficit on `r weekdayaverage[weekdayaverage$Dif == min(weekdayaverage$Dif, na.rm = TRUE),2]`, the median surplus(deficit) graph demonstrates that traffic is skewed on `r paste(weekdaymedian[weekdaymedian$Dif != 0, 2], sep = " and ")` toward a deficit. The likely reason for the difference between average and median data is either an extreme value on that day. However, again, individaul stations appear to have their own patterns. The station with the highest average deficit is station `r combinedcountbyweekday[combinedcountbyweekday$dif == min(combinedcountbyweekday$dif, na.rm = TRUE),1]` with a deficit of `r combinedcountbyweekday[combinedcountbyweekday$dif == min(combinedcountbyweekday$dif, na.rm = TRUE),5]` on `r combinedcountbyweekday[combinedcountbyweekday$dif == min(combinedcountbyweekday$dif, na.rm = TRUE),2]`. Again, there is a very clear trend that stations which have high surpluses for the weekday will continue to to have surpluses throughout the week while high deficit stations have deficits throughout the week. In this case, the overlap exists because the stations that have the highest surpluses have them across multiple days and same of deficits. This means again that these stations are good canditates to focus on by weekday with surplus stations good options for lower average bike reserves and deficit stations good options for higher average bike reserves. This also demonstrates which weekdays are particularly conducive to producing surpluses or deficits by station. The median information, indicates that most stations tend toward a deficit on those days that illustrate a median deficit. Therefore, it is worth exploring this further.


#### Wednesday Stats
```{r}
#get summary statistics for wednesday suplus(deficit)
summary(combinedcountbyweekday[combinedcountbyweekday$weekday == "Wednesday", 5])

#plot distribution of surplus and deficits by Wednesday
ggplot(data = combinedcountbyweekday[combinedcountbyweekday$weekday == "Wednesday", ], aes(x = dif)) + geom_histogram(fill = "darkgreen")+geom_vline(aes(xintercept = 0, linetype = "Zero"), color = "red") + scale_x_continuous(breaks = c(seq( from = -110, to =228, by = 15 )), labels = c(seq( from = -110, to =228, by = 15 )))
```


Looking further into Wednesday data, we see that there is a denser cluster of stations with deficits between about -30 and 0 than the surplus side. This means that there are more stations with smaller deficits than more deficit distributed over a wide range. 


### Top Surplus Stations By Weekday
```{r}
#coordinates for four are the same, introducing some random noise to separate out a bit will
  surpluscombinedcountbyweekdaycoord$start.station.latitude1 <- ifelse(duplicated(surpluscombinedcountbyweekdaycoord$start.station.latitude) == TRUE & duplicated(surpluscombinedcountbyweekdaycoord$start.station.longitude)==TRUE, jitter(surpluscombinedcountbyweekdaycoord$start.station.latitude, factor = 0.01, amount = 0.001), surpluscombinedcountbyweekdaycoord$start.station.latitude)

#make color palette
pal <- colorFactor(palette = rainbow(7), domain = surpluscombinedcountbyweekdaycoord$Weekday)
#map max surpluses by weekday
  leaflet(surpluscombinedcountbyweekdaycoord)%>%
  addTiles() %>%
  addCircleMarkers(lng = ~start.station.longitude, lat = ~start.station.latitude1, color = ~pal(Weekday),  radius = 5, opacity = 20, popup = paste("Station ID:", surpluscombinedcountbyweekdaycoord$stations, "<br>","Surplus:", surpluscombinedcountbyweekdaycoord$Surplus, "<br>", "Day:", surpluscombinedcountbyweekdaycoord[,2]))%>%
    addLegend(position = "bottomright", pal = pal, values = surpluscombinedcountbyweekdaycoord$Weekday)%>%
  setView(-73.96,40.75, zoom = 9) 
  

surpluscombinedcountbyweekdaycoord
```


This map shows which stations are the best candidates for a lower bike reserve by weekday. Please see the map for a table version of this map in case the overlap makes it difficult to discern the weekdays.




### Top Deficit Stations By Weekday
```{r}
#coordinates for four are the same, introducing some random noise to separate out a bit 
  deficitcombinedcountbyweekdaycoord$start.station.latitude1 <- ifelse(duplicated(deficitcombinedcountbyweekdaycoord$start.station.latitude) == TRUE & duplicated(deficitcombinedcountbyweekdaycoord$start.station.longitude)==TRUE, jitter(deficitcombinedcountbyweekdaycoord$start.station.latitude, factor = 0.01, amount = 0.001), deficitcombinedcountbyweekdaycoord$start.station.latitude)

#make color palette
pal <- colorFactor(palette = rainbow(7), domain = deficitcombinedcountbyweekdaycoord$Weekday)
#map deficit maxes by weekday
  leaflet(deficitcombinedcountbyweekdaycoord)%>%
  addTiles() %>%
  addCircleMarkers(lng = ~start.station.longitude, lat = ~start.station.latitude1, color = ~pal(Weekday),  radius = 5, opacity = 20, popup = paste("Station ID:", deficitcombinedcountbyweekdaycoord$stations, "<br>","Deficit:", deficitcombinedcountbyweekdaycoord$Deficit, "<br>", "Day:", deficitcombinedcountbyweekdaycoord[,2]))%>%
    addLegend(position = "bottomright", pal = pal, values = deficitcombinedcountbyweekdaycoord$Weekday)%>%
  setView(-73.96,40.75, zoom = 9) 
 

  deficitcombinedcountbyweekdaycoord
```







This map shows which stations are the best candidates to provide a higher bike reserve by weekday. Please see the map for a table version of this map in case the overlap makes it difficult to discern the weekdays. 


## Hour
```{r}
#make new df that counts start and end station by hour
startstationbyhour <- count(datafortime, start.station.id, hour)
endstationbyhour <- count(datafortime, end.station.id, hour)

#combine df's by station id and hour
combinedcountbyhour <- merge(startstationbyhour, endstationbyhour, by.x = c("start.station.id","hour"), by.y = c("end.station.id","hour"))

#find surplus(deficit) and make a new vector
combinedcountbyhour$dif <- c(combinedcountbyhour$n.y - combinedcountbyhour$n.x)

#make a plot that shows median surplus(deficit) by hour for all stations
group_by(combinedcountbyhour, hour, decreasing = TRUE)%>%
summarise(
  Dif = median(dif, na.rm = TRUE)) ->medianhour

#make df
medianhour <- as.data.frame(medianhour)
ggplot(data = medianhour, aes(x = hour, y= Dif))+geom_col(fill = "darkgreen")+xlab("Hour") + ylab("Surplus (Deficit)") +ggtitle("Median Surplus(Deficit) by Hour") 


#get count of each station for use as "index" to get a specified number of stations
#hourcountedstation <- count(combinedcountbyhour, combinedcountbyhour$start.station.id)

combinedcountbyhour[order(combinedcountbyhour$dif, decreasing = TRUE),1]-> combinedcountbyhour1 
#stations with the 5 highest surplus
topcombinedcountbyhour <- combinedcountbyhour[combinedcountbyhour$start.station.id == combinedcountbyhour1[1]| combinedcountbyhour$start.station.id == combinedcountbyhour1[2]|combinedcountbyhour$start.station.id == combinedcountbyhour1[3]|combinedcountbyhour$start.station.id == combinedcountbyhour1[4]|combinedcountbyhour$start.station.id == combinedcountbyhour1[5],]

#stations with the 5 highest deficits
bottomcombinedcountbyhour <- combinedcountbyhour[combinedcountbyhour$start.station.id == combinedcountbyhour1[length(combinedcountbyhour1)]| combinedcountbyhour$start.station.id == combinedcountbyhour1[length(combinedcountbyhour1)-1]|combinedcountbyhour$start.station.id == combinedcountbyhour1[length(combinedcountbyhour1)-2]|combinedcountbyhour$start.station.id == combinedcountbyhour1[length(combinedcountbyhour1)-3]|combinedcountbyhour$start.station.id == combinedcountbyhour1[length(combinedcountbyhour1)-4],]
#bind the rows of top and bottom 5 stations
topbottombyhour <- rbind(topcombinedcountbyhour,bottomcombinedcountbyhour)



#make a plot that shows surplus(deficit) by hour for 5 stations
#head(combinedcountbyhour, n= sum(hourcountedstation[1:5,2]))%>%
ggplot(data= topbottombyhour, aes(x= hour, y= dif))+geom_col(fill = "darkgreen")+facet_wrap("start.station.id")

#aggregate max dif by month
as.data.frame(aggregate(combinedcountbyhour$dif, by = list(Hour = combinedcountbyhour$hour), FUN = max))-> hoursurplus

#make a for loop to go through and get the stations tied to the max surplus
for(x in 1:nrow(hoursurplus)){
hoursurplus$stations[x] <- as.character(combinedcountbyhour[combinedcountbyhour$dif == hoursurplus$x[x] & combinedcountbyhour$hour == hoursurplus$Hour[x], 1])
}
#reconvert to factor
hoursurplus$stations <- as.factor(hoursurplus$stations)

#merge with lat/lon data and rename vector to "Surplus" 
unique(merge(hoursurplus, citibikesample[,c(4,6:7)], by.x = "stations", by.y = "start.station.id"))%>%rename( Surplus = x) -> hoursurplus


#aggregate min dif by hour
as.data.frame(aggregate(combinedcountbyhour$dif, by = list(Hour = combinedcountbyhour$hour), FUN = min))-> deficithour

#make a for loop to go through and get the stations tied to the largest deficits
for(x in 1:nrow(deficithour)){
deficithour$stations[x] <- as.character(combinedcountbyhour[combinedcountbyhour$dif == deficithour$x[x] & combinedcountbyhour$hour == deficithour$Hour[x], 1])
}

#reconvert to factor
deficithour$stations <- as.factor(deficithour$stations)

#merge with lat/lon data and rename vector to "Deficit" 
unique(merge(deficithour, citibikesample[,c(4,6:7)], by.x = "stations", by.y = "start.station.id"))%>%rename( Deficit = x) -> deficithour

#order by decreasing
hoursurplus <- hoursurplus[order(hoursurplus$Surplus, decreasing = TRUE),]
deficithour <- deficithour[order(deficithour$Deficit, decreasing = FALSE),]
```






Cumulative hourly data shows a work rushhour pattern. Interestingly, the morning tends to have larger deficits, indicating either that individuals are leaving at about the same time to get to work or they are leaving from the same few areas but are going to work across many areas. Conversely, the evening surplus either indicates that everyone is arriving home at around the same time or the same trend as described before but in reverse - people are going from scattered work areas and dropping bikes off at a more concentrated few places. The largest deficit is `r min(medianhour$Dif)` and it occurs at hour `r medianhour[medianhour$Dif == min(medianhour$Dif),1]`. The largest surplus is `r max(medianhour$Dif)` and it occurs at hour `r medianhour[medianhour$Dif == max(medianhour$Dif),1]`. This is important because this is a self-correcting traffic asymmetry of sorts. It would not be prudent to significantly change the bike reserves at frequent work stations because the surplus reserve from the night before is important for the morning rush. 


Once we get into hourly data by station, we see that the stations with the largest surpluses also have some of the largest deficits. We can see that for almost all of these stations, there is a large opposing flux in the evening. This is a great proxy for estimating flow of traffic. For example, the stations where we see a large deficit in the morning are likely by a large number of individuals' residence or a transit station and they are taking bikes from these to go to work while the large opposing surplus at the end of the day shows that they are returning to their home station. On the flip side, stations with a large morning surplus are likely by a large number of those individuals' work location and it is where they are dropping the bikes off. When integrating this with the average hourly data, we see that that morning commute is the most uneven. Perhaps we can extrapolate that this is because they are on average roughly evening out the asymmetric traffic incurred in the morning by returning it to the same station. The reason it likely isn't perfectly zero is because some may return to an adjacent station and there are non-business commuters taking bikes out for a ride. 




### Hour Max Surplus Plot
```{r}
#plot max surplus for each hour
ggplot(data = hoursurplus, aes(x= Hour, y = Surplus))+geom_col(fill = "darkgreen",  position = position_dodge(5)) + ggtitle("Surplus by Hour of Day")+geom_text(aes(label =  round(Surplus), vjust = -1))+ylim(0, 850)
```


Unsurpisingly, the maximum surplus occurs in the evening at station `r hoursurplus[1,1]`, hour `r hoursurplus[1,2]` while the second largest surplus occurs at station `r hoursurplus[2,1]`, hour `r hoursurplus[2,2]`. This demonstrates the pattern seen cumulatively with the median graph. These hours are likely when people are arriving at work (in the morning) and arriving at their transit station or residence (evening). The surpluses in the meantime either reflect different work schedules or leisure travel. 


### Hourly Top Surplus Stations Map
```{r}
#coordinates for four are the same, introducing some random noise to separate out a bit 
  hoursurplus$start.station.latitude1 <- ifelse(duplicated(hoursurplus$start.station.latitude) == TRUE & duplicated(hoursurplus$start.station.longitude)==TRUE, jitter(hoursurplus$start.station.latitude, factor = 0.01, amount = 0.001), hoursurplus$start.station.latitude)

#make color palette
pal <- colorFactor(palette = rainbow(24), domain = hoursurplus$Hour)
#map max surplus by hour
  leaflet(hoursurplus)%>%
  addTiles() %>%
  addCircleMarkers(lng = ~start.station.longitude, lat = ~start.station.latitude1, color = ~pal(Hour),  radius = 5, opacity = 20, popup = paste("Station ID:", hoursurplus$stations, "<br>","Surplus:", hoursurplus$Surplus, "<br>", "Hour:", hoursurplus[,2]))%>%
    addLegend(position = "bottomright", pal = pal, values = hoursurplus$Hour)%>%
  setView(-73.96,40.75, zoom = 9) 
  
  hoursurplus[order(hoursurplus$Hour, decreasing = FALSE),]
```


As a macro observation, similarly colored markers are all clustered by each other. This means that users are arriving at the same location close to each other. As expected, in the evening, some of the highest surpluses or clusters of surpluses occur around transit stations - Penn Station and Port Authority Bus Terminal. In the morning, many people are arriving at the Flatiron district on Broadway. Either this is a popular leisure spot or they are working around here.


### Hour Max Deficit Plot
```{r}
#plot max deficit for each hour
ggplot(data = deficithour, aes(x= Hour, y = Deficit))+geom_col(fill = "darkgreen",  position = position_dodge(5)) + ggtitle("Deficit by Hour of Day")+geom_text(aes(label =  -round(Deficit), vjust = +1.5))+ylim(-950,0)
```

As suspected, the hour with the largest deficit occurs at station `r deficithour[1,1]`, hour  `r deficithour[1,2]` while the second largest deficit occurs at station `r deficithour[2,1]`, hour `r deficithour[2,2]`. This demonstrates the pattern seen cumulatively with the median graph. These hours are likely when people are leaving for work and leaving work itself. The deficits in the meantime either reflect different work schedules or leisure travel. 


### Hour Deficit Stations Map
```{r}
#coordinates for four are the same, introducing some random noise to separate out a bit will
  deficithour$start.station.latitude1 <- ifelse(duplicated(deficithour$start.station.latitude) == TRUE & duplicated(deficithour$start.station.longitude)==TRUE, jitter(deficithour$start.station.latitude, factor = 0.01, amount = 0.001), deficithour$start.station.latitude)

#make color palette
pal <- colorFactor(palette = rainbow(24), domain = deficithour$Hour)
#map max deficit by hour
  leaflet(deficithour)%>%
  addTiles() %>%
  addCircleMarkers(lng = ~start.station.longitude, lat = ~start.station.latitude1, color = ~pal(Hour),  radius = 5, opacity = 20, popup = paste("Station ID:", deficithour$stations, "<br>","Deficit:", deficithour$Deficit, "<br>", "Hour:", deficithour[,2]))%>%
    addLegend(position = "bottomright", pal = pal, values = deficithour$Hour)%>%
  setView(-73.96,40.75, zoom = 9) 
  
  deficithour[order(deficithour$Hour, decreasing = FALSE),]
```



Across the board, like colors tend to be close to each other. Because we chose the rainbow pallete to illustrate by hour, this means that deficits exist around the same areas for groups of hours. Additionally, these locations largely mirror what was seen in the surplus hour map but at opposing times (morning vs evening and vice versa). The map shows that some of the highest deficit stations occur by transit sites, with one of the clusters of high deficit stations in the morning by Penn Station and another occuring by West Midtown Ferry terminal. The next highest deficit groups occur around Broadway in the Flatiron district. When considering this also appeared as a maximum surplus in the morning and there is a `r head(deficithour[deficithour$stations == "402", 2], 1) - head(hoursurplus[hoursurplus$stations == "402", 2], 1)` hour difference, this is likely a place where people work. The same is true with large surpluses spaced in the morning/evening followed by large deficits in the evening/morning. However, it is worth taking a dive into hour by weekday to see if this differs. 


### Morning Deficit Heatmap
```{r}
library(leaflet.extras)

#map the top deficit stations by hour for morning hours to mirror median trends (why we chose to only look at morning deficit to evening surplus)
leaflet(deficithour[deficithour$Hour <=9,])%>%
  addTiles() %>%
  addHeatmap(lng = ~start.station.longitude, lat = ~start.station.latitude, radius =10, intensity =   ~Deficit, minOpacity = min(abs(deficithour[deficithour$Hour <=9,3])), max = max(abs(deficithour[deficithour$Hour <=9,3])))%>%
      setView(-73.96,40.75, zoom = 12) 
```


There is a morning deficit hotspot around Penn Station indicating that this is a departure point for many in the morning.



### Evening Surplus Heatmap
```{r}
#map the top surplus stations by hour for evening hours to mirror median trends (why we chose to only look at morning deficit to evening surplus)
leaflet(hoursurplus[hoursurplus$Hour >=16,])%>%
  addTiles() %>%
  addHeatmap(lng = ~start.station.longitude, lat = ~start.station.latitude, radius =10, intensity = ~Surplus, minOpacity = min(abs(hoursurplus[hoursurplus$Hour >=16,3])), max = max(abs(hoursurplus[hoursurplus$Hour >=16,3])))%>%
      setView(-73.96,40.75, zoom = 12) 
```

 This shows that in the evening, people are arriving at Penn Station and Tompkins Park.

## Hour by Weekday
```{r warning=FALSE}
#make new df that counts start and end station by hour
startstationbyweekhour <- count(datafortime, start.station.id, weekday, hour)
endstationbyweekhour <- count(datafortime, end.station.id, weekday, hour)

#combine df's by station id and hour
combinedcountbyweekhour <- merge(startstationbyweekhour, endstationbyweekhour, by.x = c("start.station.id", "weekday", "hour"), by.y = c("end.station.id", "weekday", "hour"))


#find surplus(deficit) and make a new vector
combinedcountbyweekhour$dif <- c(combinedcountbyweekhour$n.y - combinedcountbyweekhour$n.x)

#classify by weekend/weekday
combinedcountbyweekhour$class <- ifelse(combinedcountbyweekhour$weekday == "Saturday" |combinedcountbyweekhour$weekday == "Sunday", "Weekend", "Weekday")  


#make a plot that shows median surplus(deficit) by hour for all stations
group_by(combinedcountbyweekhour, weekday, hour)%>%
summarise(
  Dif = median(dif, na.rm = TRUE)) ->medianweekhour

#make df
medianweekhour <- as.data.frame(medianweekhour)

#plot median weekhour data and split by weekday
ggplot(data = medianweekhour, aes(x = hour, y= Dif))+geom_col(fill = "darkgreen")+ facet_wrap(vars(weekday))+ xlab("Hour") + ylab("Surplus (Deficit)") +ggtitle("Median Surplus(Deficit) by Hour") 


#group by weekday and hour to summarize max deficit and the station
group_by(combinedcountbyweekhour, weekday, hour)%>%
  summarise(
    Surplus = max(dif, na.rm = TRUE),
    Station = paste(c(as.character(combinedcountbyweekhour[combinedcountbyweekhour$weekday == weekday & combinedcountbyweekhour$hour == hour & combinedcountbyweekhour$dif == max(combinedcountbyweekhour[combinedcountbyweekhour$weekday == weekday & combinedcountbyweekhour$hour == hour,6], na.rm = TRUE), 1])))) -> topsurplusweekhour
   
#group by weekday and hour to summarize max deficit and the station    
 group_by(combinedcountbyweekhour, weekday, hour)%>%
  summarise(
    Deficit = min(dif, na.rm = TRUE),
    Station = paste(c(as.character(combinedcountbyweekhour[combinedcountbyweekhour$weekday == weekday & combinedcountbyweekhour$hour == hour & combinedcountbyweekhour$dif == min(combinedcountbyweekhour[combinedcountbyweekhour$weekday == weekday & combinedcountbyweekhour$hour == hour,6], na.rm = TRUE), 1])))
  ) -> topdeficitweekhour



#bind top surplus and deficit stations
unique(rbind(topsurplusweekhour[,4], topdeficitweekhour[,4])) -> topbottomweekhour

#merge with data so match station to data
merge(topbottomweekhour, combinedcountbyweekhour[,c(1:3,6)], by.x = "Station", by.y = "start.station.id") ->  topbottombyweekhour     
topbottombyweekhour$class <- ifelse(topbottombyweekhour$weekday == "Saturday" |topbottombyweekhour$weekday == "Sunday", "Weekend", "Weekday")      


#show max surpluses and deficits by hour by weekday to show rush hours
ggplot(data= topbottombyweekhour, aes(x= hour, y= dif))+geom_col(aes(fill = class))+facet_wrap(vars(weekday)) + ggtitle("Visualization of Rush Hours")


#summary max surpluses and deficits for class
group_by(topbottombyweekhour, class)%>%
  summarise(
    Max_Surplus = max(dif, na.rm = TRUE),
    Max_Deficit = min(dif, na.rm = TRUE)
  )
```


From this graph, we can see that the weekends do not show the rush hour traffic, as predicted. Therefore, we will next identify which areas with the large surpluses and deficits that occur on weekends. Also, across the board, very few asymmetries exist between hours 0 and 5.  The summary data table also illustrates that weekdays have much larger surpluses and deficits by hour as would be expected without the rush hour traffic patterns.

### Surplus Heatmaps by Weekday and Weekend
### Surplus Heatmaps by Weekday 
```{r}
#get lat lon for stations
unique(merge(topsurplusweekhour, citibikesample[,c(4,6:7)], by.x = "Station", by.y = "start.station.id")) -> topsurplusweekhourcoord


#coordinates for four are the same, introducing some random noise to separate out a bit will
 # topsurplusweekhourcoord$start.station.latitude <- ifelse(duplicated(topsurplusweekhourcoord$start.station.latitude) == TRUE & duplicated(topsurplusweekhourcoord$start.station.longitude)==TRUE, jitter(topsurplusweekhourcoord$start.station.latitude, factor = 0.01, amount = 0.001), topsurplusweekhourcoord$start.station.latitude)
#create weekday/weekend classes  
topsurplusweekhourcoord$class <- ifelse(topsurplusweekhourcoord$weekday == "Saturday" |topsurplusweekhourcoord$weekday == "Sunday", "Weekend", "Weekday") 

#weekday map of surplus
  leaflet(topsurplusweekhourcoord[topsurplusweekhourcoord$class == "Weekday",])%>%
  addTiles() %>%
  addHeatmap(lng = ~start.station.longitude, lat = ~start.station.latitude, radius =10, intensity =   ~Surplus, minOpacity = min(topsurplusweekhourcoord[topsurplusweekhourcoord$class == "Weekday",4]), max = max(topsurplusweekhourcoord[topsurplusweekhourcoord$class == "Weekday",4]))%>%
      setView(-73.96,40.75, zoom = 12) 
```
  
  
  
  


### Surplus Heatmaps by Weekend  
```{r}  
#weekend map of surplus  
   leaflet(topsurplusweekhourcoord[topsurplusweekhourcoord$class == "Weekend",])%>%
  addTiles() %>%
  addHeatmap(lng = ~start.station.longitude, lat = ~start.station.latitude, radius =10, intensity =   ~Surplus, minOpacity = min(topsurplusweekhourcoord[topsurplusweekhourcoord$class == "Weekend",4]), max = max(topsurplusweekhourcoord[topsurplusweekhourcoord$class == "Weekend",4]))%>%
      setView(-73.96,40.75, zoom = 12) 
 
```


From this, we can see that weekday surpluses are heavily around Tompkins Square Park, Madison Square Park, Penn Station and the 42nd Port Authority Bus Terminal. Weekend surpluses are heavily around Tompkins Square Park as well but are also heavy around the south part of Central Park. This shows where users are spending most of their time visiting.


### Surplus Heatmaps by Weekday Morning and Evening
### Surplus Heatmaps by Weekday Morning 
```{r}

#map max surpluses by weekday morning
  leaflet(topsurplusweekhourcoord[topsurplusweekhourcoord$hour <= 9 & topsurplusweekhourcoord$class == "Weekday",])%>%
  addTiles() %>%
  addHeatmap(lng = ~start.station.longitude, lat = ~start.station.latitude, radius =10, intensity =   ~Surplus, minOpacity = min(topsurplusweekhourcoord[topsurplusweekhourcoord$hour <= 9 & topsurplusweekhourcoord$class == "Weekday",4]), max = max(topsurplusweekhourcoord[topsurplusweekhourcoord$hour <= 9 & topsurplusweekhourcoord$class == "Weekday",4]))%>%
      setView(-73.96,40.75, zoom = 12) 
```  





#### Surplus Heatmaps by Weekday Evening
```{r}  
#map max surpluses by weekday evening
   leaflet(topsurplusweekhourcoord[topsurplusweekhourcoord$hour >=16 & topsurplusweekhourcoord$class == "Weekday",])%>%
  addTiles() %>%
  addHeatmap(lng = ~start.station.longitude, lat = ~start.station.latitude, radius =10, intensity =   ~Surplus, minOpacity = min(topsurplusweekhourcoord[topsurplusweekhourcoord$hour >=16 & topsurplusweekhourcoord$class == "Weekday",4]), max = max(topsurplusweekhourcoord[topsurplusweekhourcoord$hour >=16 & topsurplusweekhourcoord$class == "Weekday",4]))%>%
      setView(-73.96,40.75, zoom = 12) 
 
```


In the morning, people are arriving in large amounts all over the place. However, in the evening, we can isolate five hotspots - Penn Station, the West Midtown Ferry Terminal, 42nd Port Authority Bus Terminal, Stuyvesant Town, and Tompkins Park. This means that evening surpluses are created when people are going home.


### Deficit Heatmaps by Weekday and Weekend
#### Deficit Heatmaps by Weekday 
```{r}
#get lat lon for stations
unique(merge(topdeficitweekhour, citibikesample[,c(4,6:7)], by.x = "Station", by.y = "start.station.id")) -> topdeficitweekhourcoord


#coordinates for four are the same, introducing some random noise to separate out a bit will
 # topsurplusweekhourcoord$start.station.latitude <- ifelse(duplicated(topsurplusweekhourcoord$start.station.latitude) == TRUE & duplicated(topsurplusweekhourcoord$start.station.longitude)==TRUE, jitter(topsurplusweekhourcoord$start.station.latitude, factor = 0.01, amount = 0.001), topsurplusweekhourcoord$start.station.latitude)
#create weekday/weekend classes  
topdeficitweekhourcoord$class <- ifelse(topdeficitweekhourcoord$weekday == "Saturday" |topdeficitweekhourcoord$weekday == "Sunday", "Weekend", "Weekday")  
#map max deficits by weekday
  leaflet(topdeficitweekhourcoord[topdeficitweekhourcoord$class == "Weekday",])%>%
  addTiles() %>%
  addHeatmap(lng = ~start.station.longitude, lat = ~start.station.latitude, radius =10, intensity =   ~Deficit, minOpacity = min(abs(topdeficitweekhourcoord[topdeficitweekhourcoord$class == "Weekday",4])), max = max(abs(topdeficitweekhourcoord[topdeficitweekhourcoord$class == "Weekday",4])))%>%
      setView(-73.96,40.75, zoom = 12) 
  
```  






#### Deficit Heatmaps by Weekend 
```{r}   

#map max deficits by weekend
   leaflet(topdeficitweekhourcoord[topdeficitweekhourcoord$class == "Weekend",])%>%
  addTiles() %>%
  addHeatmap(lng = ~start.station.longitude, lat = ~start.station.latitude, radius =10, intensity =   ~Deficit, minOpacity = min(abs(topdeficitweekhourcoord[topdeficitweekhourcoord$class == "Weekend",4])), max = max(abs(topdeficitweekhourcoord[topdeficitweekhourcoord$class == "Weekend",4])))%>%
      setView(-73.96,40.75, zoom = 12) 
 
```







From this, it is hard to make precise conclusions but it is clear that the geographic distribution is different on weekdays and weekends.


### Deficit Heatmaps by Weekday Morning and Evening
#### Deficit Heatmaps by Weekday Morning 
```{r}
#map max morning weekday deficits
  leaflet(topdeficitweekhourcoord[topdeficitweekhourcoord$hour <= 9 & topdeficitweekhourcoord$class == "Weekday",])%>%
  addTiles() %>%
  addHeatmap(lng = ~start.station.longitude, lat = ~start.station.latitude, radius =10, intensity =   ~Deficit, minOpacity = min(abs(topdeficitweekhourcoord[topdeficitweekhourcoord$hour <= 9 & topdeficitweekhourcoord$class == "Weekday",4])), max = max(abs(topdeficitweekhourcoord[topdeficitweekhourcoord$hour <= 9 & topdeficitweekhourcoord$class == "Weekday",4])))%>%
      setView(-73.96,40.75, zoom = 12) 
``` 



#### Deficit Heatmaps by Weekday Evening 
```{r}  
#map max evening weekday deficits
   leaflet(topdeficitweekhourcoord[topdeficitweekhourcoord$hour >=16 & topdeficitweekhourcoord$class == "Weekday",])%>%
  addTiles() %>%
  addHeatmap(lng = ~start.station.longitude, lat = ~start.station.latitude, radius =10, intensity =   ~Deficit, minOpacity = min(abs(topdeficitweekhourcoord[topdeficitweekhourcoord$hour >=16 & topdeficitweekhourcoord$class == "Weekday",4])), max = max(abs(topdeficitweekhourcoord[topdeficitweekhourcoord$hour >=16 & topdeficitweekhourcoord$class == "Weekday",4])))%>%
      setView(-73.96,40.75, zoom = 12) 
 
```

These deficit weekday morning/evening heatmaps mirror the surplus weekday morning/evening heatmaps, demonstrating that the surplus and deficits identified are largely occuring as a result of work traffic.


### Check Previous Hourly Data Against Weekday/Weekend
```{r warning=FALSE}
#merge hourly and weekhour data so can see if hourly data was impacted by weekend traffic
merge(hoursurplus, topsurplusweekhour, by.x = "stations", by.y = "Station") -> comparesurplus

#make class vector
comparesurplus$class <- ifelse(comparesurplus$weekday == "Saturday" |comparesurplus$weekday == "Sunday", "Weekend", "Weekday")



#groupby class
group_by(comparesurplus, stations)%>%
  summarise(
    Class = class
  ) -> check

#get unique rows
unique(check) -> check

#Summarize to show which are weekday, weekend, or nonexclusive stations
summarise(check,
          Type = ifelse(nrow(check[check$stations == stations,]) == 1, as.character(check[check$stations == stations, 2]), "Both")) -> checksurplus


#merge hourly and weekhour data so can see if hourly data was impacted by weekend traffic
merge(deficithour, topdeficitweekhour, by.x = "stations", by.y = "Station") -> comparedeficit

#make class vector
comparedeficit$class <- ifelse(comparedeficit$weekday == "Saturday" |comparedeficit$weekday == "Sunday", "Weekend", "Weekday")


#groupby class
group_by(comparedeficit, stations)%>%
 summarise(
    Class = class
  ) -> check

#get unique rows
unique(check) -> check

#Summarize to show which are weekday, weekend, or nonexclusive stations
summarise(check,
          Type = ifelse(nrow(check[check$stations == stations,]) == 1, as.character(check[check$stations == stations, 2]), "Both")) -> checkdeficit

 #make color pal for 
pal <- colorFactor(palette = rainbow(2), domain = topsurplusweekhourcoord$class)

#map max surplus stations by class
  leaflet(topsurplusweekhourcoord)%>%
  addTiles() %>%
  addCircleMarkers( ~start.station.longitude, ~start.station.latitude, color = ~pal(class),  radius = 5, popup = paste("Station ID:", topsurplusweekhourcoord$Station, "<br>","Surplus:", topsurplusweekhourcoord$Surplus, "<br>", "Day:", topsurplusweekhourcoord$weekday,"<br>","Hour:", topsurplusweekhourcoord$hour))%>%
    addLegend(position = "bottomright", pal = pal, values = topsurplusweekhourcoord$class)%>%
  setView(-73.96,40.75, zoom = 9) 

  
 #make color pal for 
  pal <- colorFactor(palette = rainbow(2), domain = topdeficitweekhourcoord$class)

#map max  deficit stations by class
  leaflet(topdeficitweekhourcoord)%>%
  addTiles() %>%
  addCircleMarkers( ~start.station.longitude, ~start.station.latitude, color = ~pal(class),  radius = 5, popup = paste("Station ID:", topdeficitweekhourcoord$Station, "<br>","Deficit:", topdeficitweekhourcoord$Deficit, "<br>", "Day:", topdeficitweekhourcoord$weekday,"<br>","Hour:", topdeficitweekhourcoord$hour))%>%
    addLegend(position = "bottomright", pal = pal, values = topdeficitweekhourcoord$class)%>%
  setView(-73.96,40.75, zoom = 9) 
  
#show the checks
checksurplus
checkdeficit
```


The map shows that overall, most 

When comparing  can see that station `r checkdeficit[checkdeficit$Type == "Weekend",1]` is only impacted by weekend deficits while `r checksurplus[checksurplus$Type == "Weekend",1]` is only impacted by weekend surpluses. This allows us to separate out deficits and surpluses caused by work related travel and deficits and surpluses created likely by leisure travel. While some people may be working on the weekend, we saw above that the maximum surpluses and deficits are smaller by the hour. Therefore, the weekday work travels are most important to separate out. 



### Visualize Identified Weekend Stations vs Weekday Stations for Surplus
```{r}
#coordinates for four are the same, introducing some random noise to separate out a bit will
  hoursurplus$start.station.latitude1 <- ifelse(duplicated(hoursurplus$start.station.latitude) == TRUE & duplicated(hoursurplus$start.station.longitude)==TRUE, jitter(hoursurplus$start.station.latitude, factor = 0.01, amount = 0.001), hoursurplus$start.station.latitude)

#get unique values from mapped coordinates
unique(merge(checksurplus, citibikesample[,c(4,6:7)], by.x = "stations", by.y = "start.station.id")) -> checksurpluscoord

#make weekend only
checksurpluscoord <- checksurpluscoord[checksurpluscoord$Type == "Weekend",]

#make color palette
pal <- colorFactor(palette = rainbow(24), domain = hoursurplus$Hour)

#map old hourly data with circle markers and add marker for weekend station
  leaflet()%>%
  addTiles() %>%
  addCircleMarkers(data= hoursurplus, lng = ~start.station.longitude, lat = ~start.station.latitude, color = ~pal(Hour),  radius = 5, opacity = 20, popup = paste("Station ID:", hoursurplus$stations, "<br>","Surplus:", hoursurplus$Surplus, "<br>", "Hour:", hoursurplus[,2]))%>%
    addMarkers(data = checksurpluscoord, lng = ~start.station.longitude, lat = ~start.station.latitude, popup = paste("Station ID:", checksurpluscoord$stations, "<br>","Type:", checksurpluscoord$Type) )%>%
    addLegend(position = "bottomright", pal = pal, values = hoursurplus$Hour)%>%
  setView(-73.96,40.75, zoom = 9) 
  

```

This allows us to see that `r checksurpluscoord[checksurpluscoord$Type == "Weekend",1]` is heavily impacted by weekend traffic and allows us to separate this out from the workday commute.



### Visualize Identified Weekend Stations vs Weekday Stations for Deficit
```{r}
#coordinates for some are the same, introducing some random noise to separate out a bit
  deficithour$start.station.latitude1 <- ifelse(duplicated(deficithour$start.station.latitude) == TRUE & duplicated(deficithour$start.station.longitude)==TRUE, jitter(deficithour$start.station.latitude, factor = 0.01, amount = 0.001), deficithour$start.station.latitude)

#merge with coordinates
  unique(merge(checkdeficit, citibikesample[,c(4,6:7)], by.x = "stations", by.y = "start.station.id")) -> checkdeficitcoord
  #filter for weekend only
checkdeficitcoord <- checkdeficitcoord[checkdeficitcoord$Type == "Weekend",]

#make color palette
pal <- colorFactor(palette = rainbow(24), domain = deficithour$Hour)

#map old hourly data with circle markers and add marker for weekend station
  leaflet()%>%
  addTiles() %>%
  addCircleMarkers(data = deficithour, lng = ~start.station.longitude, lat = ~start.station.latitude1, color = ~pal(Hour),  radius = 5, opacity = 20, popup = paste("Station ID:", deficithour$stations, "<br>","Deficit:", deficithour$Deficit, "<br>", "Hour:", deficithour[,2]))%>%
    addMarkers(data = checkdeficitcoord, lng = ~start.station.longitude, lat = ~start.station.latitude, popup = paste("Station ID:", checkdeficitcoord$stations, "<br>","Type:", checkdeficitcoord$Type) )%>%
    addLegend(position = "bottomright", pal = pal, values = deficithour$Hour)%>%
  setView(-73.96,40.75, zoom = 9) 
  

  

```

This allows us to see that `r checkdeficitcoord[checkdeficitcoord$Type == "Weekend",1]` is heavily impacted by weekend traffic and allows us to separate this out from the workday commute.



### Max Surplus Plot by Hour by Weekday/Weekend
```{r warning=FALSE}
#make new df that counts start and end station by hour
datafortime$class <- ifelse(datafortime$weekday == "Saturday" |datafortime$weekday == "Sunday", "Weekend", "Weekday")
startstationbyclasshour <- count(datafortime, start.station.id, class, hour)
endstationbyclasshour <- count(datafortime, end.station.id, class, hour)

#combine df's by station id class and hour
combinedcountbyclasshour <- merge(startstationbyclasshour, endstationbyclasshour, by.x = c("start.station.id", "class", "hour"), by.y = c("end.station.id", "class", "hour"))

#find surplus(deficit) and make a new vector
combinedcountbyclasshour$dif <- c(combinedcountbyclasshour$n.y - combinedcountbyclasshour$n.x)

#get max dif by class by hour and only return one station
group_by(combinedcountbyclasshour, class, hour)%>%
  summarise(
    Station = paste(head(as.character(combinedcountbyclasshour[combinedcountbyclasshour$class == class & combinedcountbyclasshour$hour == hour & combinedcountbyclasshour$dif == max(combinedcountbyclasshour[combinedcountbyclasshour$class == class & combinedcountbyclasshour$hour == hour,6], na.rm = TRUE), 1]), n=1)),
    Surplus = max(dif, na.rm = TRUE)
  ) -> classsurplushour

#merge by hour to see how weekday vs weekend contributes
merge(hoursurplus, classsurplushour, by.x = "Hour", by.y = "hour") -> comboclasssurplus



#plot max surplus for each hour by weekday/weekend
ggplot(data = comboclasssurplus, aes(x= Hour))+geom_col(fill = "darkgreen", aes(y = Surplus.x), alpha = 0.5)+ geom_col(aes(y = Surplus.y, fill = class), alpha = .5) + ggtitle("Surplus by Hour of Day with Weekday/Weekend Compared to Hourly Data")+facet_wrap(vars(class))
```

This graph shows that the hourly data was very minimally impacted by weekend noise. Anywhere the hourly data (shown in green) goes past the weekday bar, there is likely a surplus from the weekend data pushing it over while anywhere it is under the weekday bar, there is likely offsetting weekend data.



### Max Deficit Plot by Hour by Weekday/Weekend
```{r warning=FALSE}
group_by(combinedcountbyclasshour, class, hour)%>%
  summarise(
    Station = paste(head(as.character(combinedcountbyclasshour[combinedcountbyclasshour$class == class & combinedcountbyclasshour$hour == hour & combinedcountbyclasshour$dif == min(combinedcountbyclasshour[combinedcountbyclasshour$class == class & combinedcountbyclasshour$hour == hour,6], na.rm = TRUE), 1]), n=1)),
    Deficit = min(dif, na.rm = TRUE)
  ) -> classdeficithour

merge(deficithour, classdeficithour, by.x = "Hour", by.y = "hour") -> comboclassdeficit

ggplot(data = comboclassdeficit, aes(x= Hour))+geom_col(fill = "darkgreen", aes(y = Deficit.x), alpha = 0.5)+ geom_col(aes(y = Deficit.y, fill = class), alpha = 0.5) + ggtitle("Deficit by Hour of Day with Weekday/Weekend compared to Hourly Data")+facet_wrap(vars(class))
```


This graph shows that the hourly data was very minimally impacted by weekend noise. Anywhere the hourly data (shown in green) goes past the weekday bar, there is likely a deficit from the weekend data pushing it over while anywhere it is under the weekday bar, there is likely offsetting weekend data.

## Holidays
```{r}
#make new df that counts start and end station by Holiday
startstationbyholiday <- count(datafortime, start.station.id, holiday)
endstationbyholiday <- count(datafortime, end.station.id, holiday)

#combine df's by station id and holiday
combinedcountbyholiday <- merge(startstationbyholiday, endstationbyholiday, by.x = c("start.station.id","holiday"), by.y = c("end.station.id","holiday"))

#find surplus(deficit) and make a new vector
combinedcountbyholiday$dif <- c(combinedcountbyholiday$n.y - combinedcountbyholiday$n.x)

#make a plot that shows median surplus(deficit) by holiday for all stations
group_by(combinedcountbyholiday, holiday, decreasing = TRUE)%>%
summarise(
  Dif = median(dif, na.rm = TRUE))%>%
ggplot(aes(x = holiday, y= Dif))+geom_col(fill = "darkgreen")+xlab("Holiday") + ylab("Surplus (Deficit)")+ggtitle("Median Surplus(Deficit) by Holiday")+theme(axis.text.x = element_text(angle=90, hjust=1))  



combinedcountbyholiday[combinedcountbyholiday$holiday != "Not Holiday",]-> combinedcountbyholiday1 
combinedcountbyholiday1[order(combinedcountbyholiday1$dif, decreasing = TRUE),] -> combinedcountbyholiday1 
#stations with the 5 highest surplus
topcombinedcountbyholiday <- combinedcountbyholiday[combinedcountbyholiday$start.station.id == combinedcountbyholiday1[1,1]| combinedcountbyholiday$start.station.id == combinedcountbyholiday1[2,1]|combinedcountbyholiday$start.station.id == combinedcountbyholiday1[3,1]|combinedcountbyholiday$start.station.id == combinedcountbyholiday1[4,1]|combinedcountbyholiday$start.station.id == combinedcountbyholiday1[5,1],]

#stations with the 5 highest deficits
bottomcombinedcountbyholiday <- combinedcountbyholiday[combinedcountbyholiday$start.station.id == combinedcountbyholiday1[nrow(combinedcountbyholiday1), 1]| combinedcountbyholiday$start.station.id == combinedcountbyholiday1[nrow(combinedcountbyholiday1)-1, 1]|combinedcountbyholiday$start.station.id == combinedcountbyholiday1[nrow(combinedcountbyholiday1)-2, 1]|combinedcountbyholiday$start.station.id == combinedcountbyholiday1[nrow(combinedcountbyholiday1)-3, 1]|combinedcountbyholiday$start.station.id == combinedcountbyholiday1[nrow(combinedcountbyholiday1)-4, 1],]
#bind the rows of top and bottom 5 stations
topbottombyholiday <- rbind(topcombinedcountbyholiday,bottomcombinedcountbyholiday)

#make a plot that shows surplus(deficit) by holiday for 5 stations
#head(combinedcountbyholiday, n= sum(holidaycountedstation[1:5,2]))%>%
ggplot(data = topbottombyholiday, aes(x= holiday, y= dif))+geom_col(fill = "darkgreen")+facet_wrap("start.station.id")+theme(axis.text.x = element_text(angle=90, hjust=1))

#aggregate max dif by month
as.data.frame(aggregate(combinedcountbyholiday$dif, by = list(Holiday = combinedcountbyholiday$holiday), FUN = max))-> holidaysurplus

#make a for loop to go through and get the stations tied to the max surplus
for(x in 1:nrow(holidaysurplus)){
holidaysurplus$stations[x] <- as.character(combinedcountbyholiday[combinedcountbyholiday$dif == holidaysurplus$x[x] & combinedcountbyholiday$holiday == holidaysurplus$Holiday[x], 1])
}
#reconvert to factor
holidaysurplus$stations <- as.factor(holidaysurplus$stations)

#merge with lat/lon data and rename vector to "Surplus" 
unique(merge(holidaysurplus, citibikesample[,c(4,6:7)], by.x = "stations", by.y = "start.station.id"))%>%rename( Surplus = x) -> holidaysurplus


#aggregate min dif by hour
as.data.frame(aggregate(combinedcountbyholiday$dif, by = list(Holiday = combinedcountbyholiday$holiday), FUN = min))-> deficitholiday

#make a for loop to go through and get the stations tied to the largest deficits
for(x in 1:nrow(deficitholiday)){
deficitholiday$stations[x] <- as.character(combinedcountbyholiday[combinedcountbyholiday$dif == deficitholiday$x[x] & combinedcountbyholiday$holiday == deficitholiday$Holiday[x], 1])
}

#reconvert to factor
deficitholiday$stations <- as.factor(deficitholiday$stations)

#merge with lat/lon data and rename vector to "Deficit" 
unique(merge(deficitholiday, citibikesample[,c(4,6:7)], by.x = "stations", by.y = "start.station.id"))%>%rename( Deficit = x) -> deficitholiday
```










The median surplus(deficit) data holidays illustrates that there no or minimal persistent traffic asymmetries caused by holidays. Because the average data may be skewed by extreme values, it is worth looking at this median data instead to see if the problem is meaninful or due to an outlier. The largest surplus on Christmas is `r max(combinedcountbyholiday[combinedcountbyholiday$holiday == "USChristmasDay", 5], na.rm = TRUE)`, `r max(combinedcountbyholiday[combinedcountbyholiday$holiday == "USGoodFriday", 5], na.rm = TRUE)` on Good Friday, and `r max(combinedcountbyholiday[combinedcountbyholiday$holiday == "USLaborDay", 5], na.rm = TRUE)` on Labor Day. 



### Holiday Top Surplus Stations Map
```{r}
#coordinates for four are the same, introducing some random noise to separate out a bit will
  holidaysurplus$start.station.latitude <- ifelse(duplicated(holidaysurplus$start.station.latitude) == TRUE & duplicated(holidaysurplus$start.station.longitude)==TRUE, jitter(holidaysurplus$start.station.latitude, factor = 0.01, amount = 0.001), holidaysurplus$start.station.latitude)


pal <- colorFactor(palette = rainbow(length(levels(holidaysurplus$Holiday))), domain = holidaysurplus$Holiday)
#map homicides as red circles and rental properties with popup of relevant info
  leaflet(holidaysurplus)%>%
  addTiles() %>%
  addCircleMarkers(lng = ~start.station.longitude, lat = ~start.station.latitude, color = ~pal(Holiday),  radius = 5, opacity = 20, popup = paste("Station ID:", holidaysurplus$stations, "<br>","Surplus:", holidaysurplus$Surplus, "<br>", "Holiday:", holidaysurplus[,2]))%>%
    addLegend(position = "bottomright", pal = pal, values = holidaysurplus$Holiday)%>%
  setView(-73.96,40.75, zoom = 9) 
  
  holidaysurplus
```


As a macro observation, similarly colored markers are all clustered by each other. This means that users are arriving at the same location close to each other.


### Holiday Deficit Stations Map
```{r}
#coordinates for four are the same, introducing some random noise to separate out a bit will
  deficitholiday$start.station.latitude <- ifelse(duplicated(deficitholiday$start.station.latitude) == TRUE & duplicated(deficitholiday$start.station.longitude)==TRUE, jitter(deficitholiday$start.station.latitude, factor = 0.01, amount = 0.001), deficitholiday$start.station.latitude)


pal <- colorFactor(palette = rainbow(length(levels(deficitholiday$Holiday))), domain = deficitholiday$Holiday)
#map homicides as red circles and rental properties with popup of relevant info
  leaflet(deficitholiday)%>%
  addTiles() %>%
  addCircleMarkers(lng = ~start.station.longitude, lat = ~start.station.latitude, color = ~pal(Holiday),  radius = 5, opacity = 20, popup = paste("Station ID:", deficitholiday$stations, "<br>","Deficit:", deficitholiday$Deficit, "<br>", "Holiday:", deficitholiday[,2]))%>%
    addLegend(position = "bottomright", pal = pal, values = deficitholiday$Holiday)%>%
  setView(-73.96,40.75, zoom = 9) 
  
  deficitholiday
```



# Animation of Asymmetric Traffic
## Barplot
```{r}


#make an animation that shows surplus(deficit) for each day and leave a shadow 
#surplusbarplot + transition_time(day) + labs(title = "Day: {frame_time}")  + shadow_wake(wake_length = 0.1, alpha = FALSE)

```

## Line Graph
```{r}
#make an animation that draws a line graph of surplus(deficit) for each day 
surplusplot + transition_reveal(day)
```

# Dynamic Pricing Model
## Static Limits Dynamic Pricing Model
Ignoring current pricing schemes, craft a new pricing model driven by asymmetric traffic to help cope with the work-induced traffic asymmetries. Assume after analyzing price data, maximum WTP = $8 and the minimum price city bike is willing to charge is $2.  
```{r}
#count number of times per usertype per hour that a start station is used
startstationbyroundedhour <- count(datafortime, start.station.id, usertype, roundedhour)

#count number of times per usertype per hour that a end station is used
endstationbyroundedhour <- count(datafortime, end.station.id, usertype, roundedhour)

#combine df's by station id and month
combinedcountbyroundedhour <- merge(startstationbyroundedhour, endstationbyroundedhour, by.x = c("start.station.id","usertype","roundedhour"), by.y = c("end.station.id","usertype","roundedhour"))

#find surplus(deficit) and make a new vector
combinedcountbyroundedhour$dif <- c(combinedcountbyroundedhour$n.y - combinedcountbyroundedhour$n.x)

library(fitdistrplus)
#see what fits best
#descdist(combinedcountbyroundedhour$dif)
norm_dist <- fitdist(combinedcountbyroundedhour$dif, "norm")
plot(norm_dist)
#appears that there are more extreme values than expected for a normal distribution

#assume when have full data, it will be closer to a true normal distribution
#static pricing scheme using normally distributed probabilities that weight a price between $2 and $8 based on surplus - incentivize people to rent from surplus stations
combinedcountbyroundedhour$pricing <- ifelse(8*(1-pnorm(combinedcountbyroundedhour$dif, mean(combinedcountbyroundedhour$dif, na.rm = TRUE), sd(combinedcountbyroundedhour$dif, na.rm = TRUE))) < 2, 2,8*(1-pnorm(combinedcountbyroundedhour$dif, mean(combinedcountbyroundedhour$dif, na.rm = TRUE), sd(combinedcountbyroundedhour$dif, na.rm = TRUE))))

#make new df that is filtered for station id in first row
combinedcountbyroundedstation <- combinedcountbyroundedhour[combinedcountbyroundedhour$start.station.id == combinedcountbyroundedhour[1,1],]

#head(combinedcountbyroundedhour[sort(combinedcountbyroundedhour$pricing, decreasing = TRUE),]) -> topcombindedcountbyroundedhour
#tail(combinedcountbyroundedhour[sort(combinedcountbyroundedhour$pricing, decreasing = TRUE),]) -> bottomcombinedcountbyroundedhour
#rbind(topcombindedcountbyroundedhour,bottomcombinedcountbyroundedhour)

#observe how surplus(deficit) is related to pricing scheme
ggplot(combinedcountbyroundedhour, aes(x=dif, y=pricing))+geom_line(colour = "darkgreen") + xlab("Surplus(Deficit)") + ylab("Price ($)") + ggtitle("Pricing to Surplus(Deficit)") 

#observe how price changes with surplus(deficit) by hour
ggplot(combinedcountbyroundedstation, aes(x = roundedhour))+geom_line(aes(y = pricing),color = "darkgreen") + geom_line(aes(y= dif),color = "yellowgreen")+scale_y_continuous(name = "Price ($)", sec.axis = sec_axis(~.*1, name ="Surplus(Deficit)"))+theme(axis.title.y = element_text(color = "darkgreen"), axis.title.y.right = element_text(color = "yellowgreen"))+ggtitle("Price and Surplus(Deficit) Hourly") 

```










This dynamic pricing model allows City Bike to use a price scheme to help alleviate the asymmetric traffic issue that is seen during work "rush hours". By updating pricing based on the hour, City Bike would be able to charge higher prices to rent from stations with high deficits and lower prices for stations with higher surpluses. Because stations are so close together this would not inconvenience individuals too much. However, because the work rush hours tend to create a situation where bike stations near residential and transit stations have a deficit in the morning while those near work have a surplus, these stations may be spread out. Therefore, I recommend excluding the previously identified asymmetries that are created largely by work rush hours and will automatically even out. This will help to even out those stations that experience asymmetries due to fluke leisure travel. Another way to help identify these stations beyond the analysis provided above would be to offer a business customer program. These bikes could be tracked separately and used to identify these self-correcting work rush hour induced asymmetries. 
For leisure travel, users would favor stations that have surpluses and City Bike would have to use fewer resources in correcting the asymmetric traffic. Again, because stations are so close to each other and we have separated the work "rush hour" asymmetries from the leisure travel asymmetries, the evening of traffic would be beneficial and it is unlikely that City Bike would lose many customers to the price increase who would instead choose a nearby surplus station and decrease traffic asymmetries.


## Embedded Rshiny Dynamic Pricing
```{r}
#See as a separate file
#allow user to change bounds of pricing scheme using rshiny
#write.csv(combinedcountbyroundedhour,"Rshinydynamicpricing.csv")
#library(shiny)
#shinyApp(

  #create user interface with min price and max price inputs and two outputs - one a plot of surplus to price and another a table of prices and related data given pricing criteria
 # ui <- fluidPage(
      #numericInput("MinPrice", "Enter Your Minimum Price Here", 2),
      #numericInput("MaxPrice", "Enter Maximum Price Here", 8), 
      #plotOutput("results"),
    #  dataTableOutput("list")
   # ),
  
  #create server
  #server <- function(input, output){
    
  #create reactive variable pricing that reacts to changing price input criteria
  #pricing <- reactive({
    
   #combinedcountbyroundedhour$pricing <- ifelse(input$MaxPrice*(1-pnorm(combinedcountbyroundedhour$dif, mean(combinedcountbyroundedhour$dif, na.rm = TRUE), sd(combinedcountbyroundedhour$dif, na.rm = TRUE))) < input$MinPrice, input$MinPrice,input$MaxPrice*(1-pnorm(combinedcountbyroundedhour$dif, mean(combinedcountbyroundedhour$dif, na.rm = TRUE), sd(combinedcountbyroundedhour$dif, na.rm = TRUE))))
  
 # })
  
  #specify output plot - shows line graph of surplus(deficit) to pricing reactive variable
 # output$results <- renderPlot({
  #ggplot(combinedcountbyroundedhour,aes(x=dif, y=pricing()))+geom_line()
 #   })
  
  #specify output data table - shows pricing for changing price inputs
 # output$list <- renderDataTable({
   # cbind(pricing(), combinedcountbyroundedhour)
    
 # })
#},

#because embedding in rmarkdown, creating enough space for it to display
#options = list(height = 500)
#)
```

# Follow Bike
Follow bike 35051.
```{r}


#make new df with relevant vectors from sample
biketravels <- citibikesample[,c(2:4,6:8,10:12)]

#make uniquebike df which is for bikeid 26483 and arrange by increasing time
biketravels[sort(biketravels$starttime, descending = FALSE),] ->l
l[l$bikeid == 35051,] -> uniquebike

#format as ymdhms
uniquebike$starttime <- floor_date(ymd_hms(uniquebike$starttime),"minute")
uniquebike$stoptime <- floor_date(ymd_hms(uniquebike$stoptime),"minute")

#for loop to add moved to anywhere the end station is not the next start station
for (x in 1:nrow(uniquebike)) {
  uniquebike$travel[x] <- ifelse(uniquebike[x+1,3] == uniquebike[x,6],"Not Moved","Moved")
} 

#make df's for start and end lat/lon and times
#startlocationlat <- as.data.frame(uniquebike[,4])
#endlocationlat <- as.data.frame(uniquebike[,7])
#startlocationlon <- as.data.frame(uniquebike[,5])
#endlocationlon <- as.data.frame(uniquebike[,8])
#starttime <- as.data.frame(uniquebike[,1])
#endtime <- as.data.frame(uniquebike[,2])

library("schoolmath")

#define a as twice the length of vector so will go until both are entirely weaved
#a <- 2*nrow(startlocationlat)

#make thing an empty df with doubles 
#thing <- data.frame(Lat = double())

#make for loop to weave start and end lats together so that end is after start and store in df
#for (x in 1:a) {
#  ifelse(x==1, thing[x,1] <- startlocationlat[x,1],ifelse(is.even(x) == FALSE, thing[x,1]<-startlocationlat[(x-((x-1)/2)),1], thing[x,1] <- endlocationlat[(x-(x/2)),1]))
#}  

#define b as twice the length of vector so will go until both are entirely weaved
#b <- 2*nrow(startlocationlon)

#make thing1 an empty df with doubles
#thing1 <- data.frame(Lon = double())

#make for loop to weave start and end lons together so that end is after start and store in df
#for (x in 1:b) {
#  ifelse(x==1, thing1[x,1] <- startlocationlon[x,1],ifelse(is.even(x) == FALSE, thing1[x,1]<-startlocationlon[(x-((x-1)/2)),1], thing1[x,1] <- endlocationlon[(x-(x/2)),1]))
#} 

#define c as twice the length of vector so will go until both are entirely weaved
#c <- 2*nrow(starttime)

#make timing an empty df with date format
#timing <- data.frame(Time = POSIXct())

#make for loop to weave start and end times together so that end is after start and store in df
#for (x in 1:c) {
 # ifelse(x==1, timing[x,1] <- starttime[x,1],ifelse(is.even(x) == FALSE, timing[x,1]<-starttime[(x-((x-1)/2)),1], timing[x,1] <- endtime[(x-(x/2)),1]))
#}  

#keep only first vector which is data that has been weaved together
#finaltravellat <- data.frame(thing[,1])
#finaltravellon <- data.frame(thing1[,1])
#finaltime <- data.frame(timing[,1])

#combine weaved vectors into df
#cbind(finaltime, finaltravellat, finaltravellon) -> pathbike

#get rid of duplicates
#pathbikeunique <- unique(pathbike[order(pathbike$timing...1., decreasing = FALSE),])

#get map for nyc
#citymap1 <- get_stamenmap(bbox = c(left = -74.03, bottom = 40.65, right = -73.87, top = 40.87), zoom = 12, maptype = c("terrain")) 

#display map and overlay point representing bike and display animation by minute to track bike's movement and get a sense for the speed it traveled
#ggmap(citymap1) + geom_point(data=pathbikeunique, aes(x= thing1...1., y= thing...1.), colour = "blue", alpha = 0.5, size = 2)+ transition_reveal(along = pathbikeunique$timing...1.)+labs(title = "Date: {frame_along}")-> a
#animate(a, duration = 91, fps = 20) ->smoothanimation
gif_file("biketravels.gif")
#anim_save("biketravels.gif", animation = last_animation())

```










The animation allows us to visualize the path of the most traveled bike in our sample. Through this, we are able to get a sense for how frequently the bike is moved, where the bike tends to remain around, and relative speeds of travel.


```{r}
#clearing up space
list <- ls()
citibikesample1 <- citibikesample
rm(list = list)
rm(list)


```






















